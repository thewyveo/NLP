{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67ff92c",
   "metadata": {},
   "source": [
    "#  Named Entity Recognition and Classification (NERC) Analysis\n",
    "\n",
    "# NLP Final Project - NERC Component\n",
    "\n",
    "\n",
    "**Systems to Compare**:\n",
    "- System 1: spaCy's transformer-based model (`en_core_web_trf`)\n",
    "- System 2: BERT-based model (`dslim/bert-base-NER`) via Hugging Face\n",
    "\n",
    "**Training Datasets**:\n",
    "- CoNLL-2003\n",
    "- WikiANN (English)\n",
    "- WNUT-17\n",
    "\n",
    "**Evaluation**: Performance comparison using precision, recall, F1-score, and error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4a2a3",
   "metadata": {},
   "source": [
    " Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428af006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token_id     token BIO_NER_tag\n",
      "0         0        If           O\n",
      "1         1    you're           O\n",
      "2         2  visiting           O\n",
      "3         3     Paris  B-LOCATION\n",
      "4         4         ,           O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, \n",
    "    TrainingArguments, DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "import random\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('datasets/NER-test.tsv', sep='\\t')\n",
    "test_data.drop('sentence_id', axis=1, inplace=True)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f8b3e",
   "metadata": {},
   "source": [
    "Data Set Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a721c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDatasetLoader:\n",
    "    def __init__(self):\n",
    "        self.datasets = {}\n",
    "        self.label_mappings = {}\n",
    "    \n",
    "    def load_conll2003(self):\n",
    "        \"\"\"Load CoNLL-2003 dataset\"\"\"\n",
    "        print(\"Loading CoNLL-2003 dataset...\")\n",
    "        dataset = load_dataset(\"conll2003\")\n",
    "        \n",
    "        # Extract labels\n",
    "        labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "        self.label_mappings['conll2003'] = {i: label for i, label in enumerate(labels)}\n",
    "        \n",
    "        self.datasets['conll2003'] = {\n",
    "            'train': dataset['train'],\n",
    "            'validation': dataset['validation'],\n",
    "            'test': dataset['test'],\n",
    "            'labels': labels\n",
    "        }\n",
    "        print(f\"CoNLL-2003 loaded. Labels: {labels}\")\n",
    "        return self.datasets['conll2003']\n",
    "    \n",
    "    def load_wikiann(self, language='en'):\n",
    "        \"\"\"Load WikiANN dataset for English\"\"\"\n",
    "        print(f\"Loading WikiANN ({language}) dataset...\")\n",
    "        dataset = load_dataset(\"wikiann\", language)\n",
    "        \n",
    "        # Extract labels\n",
    "        labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "        self.label_mappings['wikiann'] = {i: label for i, label in enumerate(labels)}\n",
    "        \n",
    "        self.datasets['wikiann'] = {\n",
    "            'train': dataset['train'],\n",
    "            'validation': dataset['validation'],\n",
    "            'test': dataset['test'],\n",
    "            'labels': labels\n",
    "        }\n",
    "        print(f\"WikiANN ({language}) loaded. Labels: {labels}\")\n",
    "        return self.datasets['wikiann']\n",
    "    \n",
    "    def load_wnut17(self):\n",
    "        \"\"\"Load WNUT-17 dataset\"\"\"\n",
    "        print(\"Loading WNUT-17 dataset...\")\n",
    "        dataset = load_dataset(\"wnut_17\")\n",
    "        \n",
    "        # Extract labels\n",
    "        labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "        self.label_mappings['wnut17'] = {i: label for i, label in enumerate(labels)}\n",
    "        \n",
    "        self.datasets['wnut17'] = {\n",
    "            'train': dataset['train'],\n",
    "            'validation': dataset['validation'],\n",
    "            'test': dataset['test'],\n",
    "            'labels': labels\n",
    "        }\n",
    "        print(f\"WNUT-17 loaded. Labels: {labels}\")\n",
    "        return self.datasets['wnut17']\n",
    "    \n",
    "    def get_combined_labels(self):\n",
    "        \"\"\"Get all unique labels across datasets\"\"\"\n",
    "        all_labels = set()\n",
    "        for dataset_name, mapping in self.label_mappings.items():\n",
    "            all_labels.update(mapping.values())\n",
    "        return sorted(list(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d6f33",
   "metadata": {},
   "source": [
    "**Data preprocessing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14280a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataPreprocessor:\n",
    "    def __init__(self, tokenizer_name=\"bert-base-cased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "    def align_labels_with_tokens(self, labels, word_ids):\n",
    "        \"\"\"Align labels with tokenized words\"\"\"\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id is None:\n",
    "                new_labels.append(-100)\n",
    "            else:\n",
    "                label = labels[word_id]\n",
    "                if label % 2 == 1:  # If it's an I- tag\n",
    "                    new_labels.append(label)\n",
    "                else:  # If it's a B- tag, change to I-\n",
    "                    new_labels.append(label + 1 if label != 0 else 0)\n",
    "        return new_labels\n",
    "    \n",
    "    def tokenize_and_align_labels(self, examples, label_all_tokens=True):\n",
    "        \"\"\"Tokenize and align labels for BERT-style models\"\"\"\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            examples[\"tokens\"], \n",
    "            truncation=True, \n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            aligned_labels = self.align_labels_with_tokens(label, word_ids)\n",
    "            labels.append(aligned_labels)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    def prepare_spacy_data(self, dataset):\n",
    "        \"\"\"Prepare data for spaCy training\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for example in dataset:\n",
    "            tokens = example['tokens']\n",
    "            ner_tags = example['ner_tags']\n",
    "            \n",
    "            # Convert to spaCy format\n",
    "            entities = []\n",
    "            start_pos = 0\n",
    "            \n",
    "            for i, (token, tag) in enumerate(zip(tokens, ner_tags)):\n",
    "                if tag != 0:  # Not 'O' tag\n",
    "                    tag_name = dataset.features['ner_tags'].feature.names[tag]\n",
    "                    if tag_name.startswith('B-'):\n",
    "                        entity_start = start_pos\n",
    "                        entity_label = tag_name[2:]\n",
    "                        entity_end = start_pos + len(token)\n",
    "                        \n",
    "                        # Check for I- tags following this B- tag\n",
    "                        j = i + 1\n",
    "                        while j < len(ner_tags) and dataset.features['ner_tags'].feature.names[ner_tags[j]].startswith(f'I-{entity_label}'):\n",
    "                            entity_end = start_pos + len(' '.join(tokens[i:j+1]))\n",
    "                            j += 1\n",
    "                        \n",
    "                        entities.append((entity_start, entity_end, entity_label))\n",
    "                \n",
    "                start_pos += len(token) + 1  # +1 for space\n",
    "            \n",
    "            text = ' '.join(tokens)\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "        \n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad70bf",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Exploration\n",
    "\n",
    "### 2.1 Load Training Datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32beb495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# CoNLL-2003 dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m conll_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconll2003\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ CoNLL-2003 loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconll_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# WikiANN (English) dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load datasets from Hugging Face\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# CoNLL-2003 dataset\n",
    "conll_dataset = load_dataset(\"conll2003\")\n",
    "print(f\"‚úÖ CoNLL-2003 loaded: {conll_dataset}\")\n",
    "\n",
    "# WikiANN (English) dataset\n",
    "wikiann_dataset = load_dataset(\"wikiann\", \"en\")\n",
    "print(f\"‚úÖ WikiANN (English) loaded: {wikiann_dataset}\")\n",
    "\n",
    "# WNUT-17 dataset\n",
    "wnut_dataset = load_dataset(\"wnut_17\")\n",
    "print(f\"‚úÖ WNUT-17 loaded: {wnut_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d48376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration function\n",
    "def explore_dataset(dataset, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä {name} Dataset Statistics\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Basic info\n",
    "    train_size = len(dataset['train']) if 'train' in dataset else 0\n",
    "    test_size = len(dataset['test']) if 'test' in dataset else 0\n",
    "    val_size = len(dataset['validation']) if 'validation' in dataset else 0\n",
    "    \n",
    "    print(f\"Train samples: {train_size:,}\")\n",
    "    print(f\"Test samples: {test_size:,}\")\n",
    "    print(f\"Validation samples: {val_size:,}\")\n",
    "    \n",
    "    # Example sample\n",
    "    if 'train' in dataset and len(dataset['train']) > 0:\n",
    "        example = dataset['train'][0]\n",
    "        print(f\"\\nüìù Example sample:\")\n",
    "        print(f\"Tokens: {example['tokens'][:10]}...\")\n",
    "        print(f\"NER tags: {example['ner_tags'][:10]}...\")\n",
    "        \n",
    "        # Get unique NER tags\n",
    "        if 'train' in dataset:\n",
    "            all_tags = []\n",
    "            for sample in dataset['train']:\n",
    "                all_tags.extend(sample['ner_tags'])\n",
    "            unique_tags = set(all_tags)\n",
    "            print(f\"\\nüè∑Ô∏è  Unique NER tags: {len(unique_tags)}\")\n",
    "            print(f\"Tag distribution: {Counter(all_tags).most_common(10)}\")\n",
    "\n",
    "# Explore each dataset\n",
    "explore_dataset(conll_dataset, \"CoNLL-2003\")\n",
    "explore_dataset(wikiann_dataset, \"WikiANN\")\n",
    "explore_dataset(wnut_dataset, \"WNUT-17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37898c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NER tag mappings\n",
    "def get_tag_mapping(dataset, dataset_name):\n",
    "    \"\"\"Extract NER tag mappings from dataset features\"\"\"\n",
    "    try:\n",
    "        if 'train' in dataset:\n",
    "            features = dataset['train'].features\n",
    "            if 'ner_tags' in features:\n",
    "                tag_names = features['ner_tags'].feature.names\n",
    "                return {i: tag for i, tag in enumerate(tag_names)}\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Get tag mappings for each dataset\n",
    "conll_tags = get_tag_mapping(conll_dataset, \"CoNLL-2003\")\n",
    "wikiann_tags = get_tag_mapping(wikiann_dataset, \"WikiANN\")\n",
    "wnut_tags = get_tag_mapping(wnut_dataset, \"WNUT-17\")\n",
    "\n",
    "print(\"üè∑Ô∏è  Tag Mappings:\")\n",
    "print(f\"CoNLL-2003: {conll_tags}\")\n",
    "print(f\"WikiANN: {wikiann_tags}\")\n",
    "print(f\"WNUT-17: {wnut_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914afe8f",
   "metadata": {},
   "source": [
    "## 3. Dataset Preprocessing and Format Standardization\n",
    "\n",
    "### 3.1 Convert to Standard BIO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44725480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(dataset, tag_mapping, dataset_name):\n",
    "    \"\"\"Convert dataset to standard BIO format\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    if 'train' in dataset:\n",
    "        for sample in dataset['train']:\n",
    "            tokens = sample['tokens']\n",
    "            ner_tags = sample['ner_tags']\n",
    "            \n",
    "            # Convert numeric tags to string labels\n",
    "            if tag_mapping:\n",
    "                bio_tags = [tag_mapping[tag] for tag in ner_tags]\n",
    "            else:\n",
    "                bio_tags = [str(tag) for tag in ner_tags]\n",
    "            \n",
    "            processed_data.append({\n",
    "                'tokens': tokens,\n",
    "                'ner_tags': bio_tags,\n",
    "                'sentence': ' '.join(tokens)\n",
    "            })\n",
    "    \n",
    "    return processed_data[:1000]  # Limit for demo purposes\n",
    "\n",
    "# Process datasets\n",
    "conll_processed = convert_to_bio_format(conll_dataset, conll_tags, \"CoNLL-2003\")\n",
    "wikiann_processed = convert_to_bio_format(wikiann_dataset, wikiann_tags, \"WikiANN\")\n",
    "wnut_processed = convert_to_bio_format(wnut_dataset, wnut_tags, \"WNUT-17\")\n",
    "\n",
    "print(f\"‚úÖ Processed datasets:\")\n",
    "print(f\"CoNLL-2003: {len(conll_processed)} samples\")\n",
    "print(f\"WikiANN: {len(wikiann_processed)} samples\")\n",
    "print(f\"WNUT-17: {len(wnut_processed)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from processed data\n",
    "print(\"üìù Example processed samples:\")\n",
    "print(\"\\nüîπ CoNLL-2003 Example:\")\n",
    "if conll_processed:\n",
    "    example = conll_processed[0]\n",
    "    print(f\"Tokens: {example['tokens'][:10]}\")\n",
    "    print(f\"NER Tags: {example['ner_tags'][:10]}\")\n",
    "    print(f\"Sentence: {example['sentence'][:100]}...\")\n",
    "\n",
    "print(\"\\nüîπ WikiANN Example:\")\n",
    "if wikiann_processed:\n",
    "    example = wikiann_processed[0]\n",
    "    print(f\"Tokens: {example['tokens'][:10]}\")\n",
    "    print(f\"NER Tags: {example['ner_tags'][:10]}\")\n",
    "    print(f\"Sentence: {example['sentence'][:100]}...\")\n",
    "\n",
    "print(\"\\nüîπ WNUT-17 Example:\")\n",
    "if wnut_processed:\n",
    "    example = wnut_processed[0]\n",
    "    print(f\"Tokens: {example['tokens'][:10]}\")\n",
    "    print(f\"NER Tags: {example['ner_tags'][:10]}\")\n",
    "    print(f\"Sentence: {example['sentence'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8ec3a",
   "metadata": {},
   "source": [
    "## 4. System 1: spaCy Transformer-based NER\n",
    "\n",
    "### 4.1 Load spaCy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd171dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy transformer model\n",
    "print(\"Loading spaCy transformer model...\")\n",
    "try:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_trf\")\n",
    "    print(\"‚úÖ spaCy en_core_web_trf model loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"‚ùå spaCy model not found. Installing...\")\n",
    "    os.system(\"python -m spacy download en_core_web_trf\")\n",
    "    nlp_spacy = spacy.load(\"en_core_web_trf\")\n",
    "    print(\"‚úÖ spaCy model installed and loaded!\")\n",
    "\n",
    "# Test spaCy model\n",
    "test_text = \"Apple Inc. is based in Cupertino, California. Tim Cook is the CEO.\"\n",
    "doc = nlp_spacy(test_text)\n",
    "\n",
    "print(f\"\\nüß™ Testing spaCy on: '{test_text}'\")\n",
    "print(\"Entities found:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  {ent.text} -> {ent.label_} ({ent.start_char}-{ent.end_char})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_spacy(texts: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Process texts with spaCy NER and return BIO tags\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        doc = nlp_spacy(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        ner_tags = []\n",
    "        \n",
    "        # Convert spaCy entities to BIO format\n",
    "        for token in doc:\n",
    "            if token.ent_type_:\n",
    "                if token.ent_iob_ == 'B':\n",
    "                    ner_tags.append(f\"B-{token.ent_type_}\")\n",
    "                elif token.ent_iob_ == 'I':\n",
    "                    ner_tags.append(f\"I-{token.ent_type_}\")\n",
    "                else:\n",
    "                    ner_tags.append('O')\n",
    "            else:\n",
    "                ner_tags.append('O')\n",
    "        \n",
    "        results.append(ner_tags)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "test_sentences = [\n",
    "    \"Apple Inc. is based in Cupertino, California.\",\n",
    "    \"Tim Cook met with Elon Musk in New York.\",\n",
    "    \"Microsoft announced new features yesterday.\"\n",
    "]\n",
    "\n",
    "spacy_results = process_with_spacy(test_sentences)\n",
    "print(\"üß™ spaCy NER Results:\")\n",
    "for i, (sentence, tags) in enumerate(zip(test_sentences, spacy_results)):\n",
    "    print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "    print(f\"Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5f29c",
   "metadata": {},
   "source": [
    "## 5. System 2: BERT-based NER with Hugging Face\n",
    "\n",
    "### 5.1 Load BERT NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae286b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT-based NER model\n",
    "print(\"Loading BERT NER model...\")\n",
    "bert_ner = pipeline(\"ner\", \n",
    "                   model=\"dslim/bert-base-NER\", \n",
    "                   tokenizer=\"dslim/bert-base-NER\",\n",
    "                   aggregation_strategy=\"simple\")\n",
    "\n",
    "print(\"‚úÖ BERT NER model loaded successfully!\")\n",
    "\n",
    "# Test BERT model\n",
    "test_text = \"Apple Inc. is based in Cupertino, California. Tim Cook is the CEO.\"\n",
    "bert_results = bert_ner(test_text)\n",
    "\n",
    "print(f\"\\nüß™ Testing BERT NER on: '{test_text}'\")\n",
    "print(\"Entities found:\")\n",
    "for entity in bert_results:\n",
    "    print(f\"  {entity['word']} -> {entity['entity_group']} (confidence: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_bert(texts: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Process texts with BERT NER and return BIO tags\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Get entities from BERT\n",
    "        entities = bert_ner(text)\n",
    "        \n",
    "        # Tokenize text (simple whitespace tokenization for alignment)\n",
    "        tokens = text.split()\n",
    "        ner_tags = ['O'] * len(tokens)\n",
    "        \n",
    "        # Convert BERT entities to BIO format\n",
    "        for entity in entities:\n",
    "            entity_text = entity['word'].replace('##', '')  # Remove BERT subword markers\n",
    "            entity_label = entity['entity_group']\n",
    "            \n",
    "            # Find the token(s) that match this entity\n",
    "            for i, token in enumerate(tokens):\n",
    "                if entity_text.lower() in token.lower() or token.lower() in entity_text.lower():\n",
    "                    if ner_tags[i] == 'O':  # Only set if not already tagged\n",
    "                        ner_tags[i] = f\"B-{entity_label}\"\n",
    "                    break\n",
    "        \n",
    "        results.append(ner_tags)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "bert_bio_results = process_with_bert(test_sentences)\n",
    "print(\"üß™ BERT NER BIO Results:\")\n",
    "for i, (sentence, tags) in enumerate(zip(test_sentences, bert_bio_results)):\n",
    "    print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "    print(f\"Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266ff21",
   "metadata": {},
   "source": [
    "## 6. Test Data Processing\n",
    "\n",
    "### 6.1 Create Test Dataset (Placeholder for NER-test.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder test data (replace with actual NER-test.tsv when available)\n",
    "test_data = {\n",
    "    'sentence_id': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n",
    "    'token': ['Apple', 'Inc.', 'announced', 'new', 'features', 'Tim', 'Cook', 'visited', 'New', 'York', 'Microsoft', 'acquired', 'a', 'startup', 'yesterday'],\n",
    "    'true_label': ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC', 'B-ORG', 'O', 'O', 'O', 'O']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(\"üìä Test Data (Placeholder):\")\n",
    "print(test_df)\n",
    "\n",
    "# Group by sentence\n",
    "test_sentences_df = test_df.groupby('sentence_id').agg({\n",
    "    'token': lambda x: list(x),\n",
    "    'true_label': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Create sentence texts\n",
    "test_sentences_df['sentence'] = test_sentences_df['token'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"\\nüìù Test Sentences:\")\n",
    "for idx, row in test_sentences_df.iterrows():\n",
    "    print(f\"Sentence {row['sentence_id']}: {row['sentence']}\")\n",
    "    print(f\"True labels: {row['true_label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply both NER systems to test sentences\n",
    "test_sentences_list = test_sentences_df['sentence'].tolist()\n",
    "\n",
    "print(\"üîÑ Applying NER systems to test data...\")\n",
    "\n",
    "# System 1: spaCy\n",
    "spacy_predictions = process_with_spacy(test_sentences_list)\n",
    "print(\"‚úÖ spaCy predictions completed\")\n",
    "\n",
    "# System 2: BERT\n",
    "bert_predictions = process_with_bert(test_sentences_list)\n",
    "print(\"‚úÖ BERT predictions completed\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_data = []\n",
    "for i, row in test_sentences_df.iterrows():\n",
    "    sentence_id = row['sentence_id']\n",
    "    tokens = row['token']\n",
    "    true_labels = row['true_label']\n",
    "    spacy_pred = spacy_predictions[i] if i < len(spacy_predictions) else ['O'] * len(tokens)\n",
    "    bert_pred = bert_predictions[i] if i < len(bert_predictions) else ['O'] * len(tokens)\n",
    "    \n",
    "    # Ensure all lists have the same length\n",
    "    max_len = max(len(tokens), len(true_labels), len(spacy_pred), len(bert_pred))\n",
    "    tokens.extend([''] * (max_len - len(tokens)))\n",
    "    true_labels.extend(['O'] * (max_len - len(true_labels)))\n",
    "    spacy_pred.extend(['O'] * (max_len - len(spacy_pred)))\n",
    "    bert_pred.extend(['O'] * (max_len - len(bert_pred)))\n",
    "    \n",
    "    for j in range(len(tokens)):\n",
    "        if tokens[j]:  # Only add non-empty tokens\n",
    "            results_data.append({\n",
    "                'sentence_id': sentence_id,\n",
    "                'token': tokens[j],\n",
    "                'true_label': true_labels[j],\n",
    "                'spacy_pred': spacy_pred[j],\n",
    "                'bert_pred': bert_pred[j]\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nüìä Prediction Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e5226",
   "metadata": {},
   "source": [
    "## 7. Performance Evaluation and Metrics\n",
    "\n",
    "### 7.1 Calculate NER Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for seqeval evaluation\n",
    "def prepare_for_evaluation(df):\n",
    "    \"\"\"Group predictions by sentence for seqeval\"\"\"\n",
    "    grouped = df.groupby('sentence_id')\n",
    "    \n",
    "    true_labels = []\n",
    "    spacy_preds = []\n",
    "    bert_preds = []\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        true_labels.append(group['true_label'].tolist())\n",
    "        spacy_preds.append(group['spacy_pred'].tolist())\n",
    "        bert_preds.append(group['bert_pred'].tolist())\n",
    "    \n",
    "    return true_labels, spacy_preds, bert_preds\n",
    "\n",
    "true_labels, spacy_preds, bert_preds = prepare_for_evaluation(results_df)\n",
    "\n",
    "print(\"üìä Evaluation Data Prepared:\")\n",
    "print(f\"Number of sentences: {len(true_labels)}\")\n",
    "print(f\"True labels example: {true_labels[0]}\")\n",
    "print(f\"spaCy predictions example: {spacy_preds[0]}\")\n",
    "print(f\"BERT predictions example: {bert_preds[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe988f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for both systems\n",
    "print(\"üîç Calculating NER Metrics...\")\n",
    "\n",
    "# spaCy Metrics\n",
    "try:\n",
    "    spacy_f1 = f1_score(true_labels, spacy_preds)\n",
    "    spacy_precision = precision_score(true_labels, spacy_preds)\n",
    "    spacy_recall = recall_score(true_labels, spacy_preds)\n",
    "    spacy_report = classification_report(true_labels, spacy_preds, digits=4)\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating spaCy metrics: {e}\")\n",
    "    spacy_f1 = spacy_precision = spacy_recall = 0.0\n",
    "    spacy_report = \"Error in calculation\"\n",
    "\n",
    "# BERT Metrics\n",
    "try:\n",
    "    bert_f1 = f1_score(true_labels, bert_preds)\n",
    "    bert_precision = precision_score(true_labels, bert_preds)\n",
    "    bert_recall = recall_score(true_labels, bert_preds)\n",
    "    bert_report = classification_report(true_labels, bert_preds, digits=4)\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating BERT metrics: {e}\")\n",
    "    bert_f1 = bert_precision = bert_recall = 0.0\n",
    "    bert_report = \"Error in calculation\"\n",
    "\n",
    "print(\"\\nüìà METRICS SUMMARY:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'System':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'spaCy':<15} {spacy_precision:<12.4f} {spacy_recall:<12.4f} {spacy_f1:<12.4f}\")\n",
    "print(f\"{'BERT':<15} {bert_precision:<12.4f} {bert_recall:<12.4f} {bert_f1:<12.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Detailed reports\n",
    "print(f\"\\nüîç spaCy Detailed Report:\")\n",
    "print(spacy_report)\n",
    "\n",
    "print(f\"\\nüîç BERT Detailed Report:\")\n",
    "print(bert_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "metrics_data = {\n",
    "    'System': ['spaCy', 'BERT'],\n",
    "    'Precision': [spacy_precision, bert_precision],\n",
    "    'Recall': [spacy_recall, bert_recall],\n",
    "    'F1-Score': [spacy_f1, bert_f1]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Create bar plot\n",
    "fig = make_subplots(rows=1, cols=3, \n",
    "                    subplot_titles=('Precision', 'Recall', 'F1-Score'),\n",
    "                    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]])\n",
    "\n",
    "# Add bars for each metric\n",
    "fig.add_trace(go.Bar(x=metrics_df['System'], y=metrics_df['Precision'], \n",
    "                     name='Precision', marker_color='lightblue'), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=metrics_df['System'], y=metrics_df['Recall'], \n",
    "                     name='Recall', marker_color='lightgreen'), row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=metrics_df['System'], y=metrics_df['F1-Score'], \n",
    "                     name='F1-Score', marker_color='lightcoral'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(title_text=\"NER Systems Performance Comparison\", showlegend=False)\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()\n",
    "\n",
    "# Summary table for poster\n",
    "print(\"\\nüìã SUMMARY TABLE FOR POSTER:\")\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a7287",
   "metadata": {},
   "source": [
    "## 8. Results Comparison and Analysis\n",
    "\n",
    "### 8.1 Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison analysis\n",
    "print(\"üîç DETAILED COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze agreement/disagreement\n",
    "agreement_analysis = []\n",
    "for i, row in results_df.iterrows():\n",
    "    token = row['token']\n",
    "    true_label = row['true_label']\n",
    "    spacy_pred = row['spacy_pred']\n",
    "    bert_pred = row['bert_pred']\n",
    "    \n",
    "    spacy_correct = spacy_pred == true_label\n",
    "    bert_correct = bert_pred == true_label\n",
    "    systems_agree = spacy_pred == bert_pred\n",
    "    \n",
    "    agreement_analysis.append({\n",
    "        'token': token,\n",
    "        'true_label': true_label,\n",
    "        'spacy_pred': spacy_pred,\n",
    "        'bert_pred': bert_pred,\n",
    "        'spacy_correct': spacy_correct,\n",
    "        'bert_correct': bert_correct,\n",
    "        'systems_agree': systems_agree,\n",
    "        'both_correct': spacy_correct and bert_correct,\n",
    "        'both_wrong': not spacy_correct and not bert_correct,\n",
    "        'spacy_only_correct': spacy_correct and not bert_correct,\n",
    "        'bert_only_correct': bert_correct and not spacy_correct\n",
    "    })\n",
    "\n",
    "agreement_df = pd.DataFrame(agreement_analysis)\n",
    "\n",
    "# Calculate agreement statistics\n",
    "total_tokens = len(agreement_df)\n",
    "systems_agree_count = agreement_df['systems_agree'].sum()\n",
    "both_correct_count = agreement_df['both_correct'].sum()\n",
    "both_wrong_count = agreement_df['both_wrong'].sum()\n",
    "spacy_only_correct_count = agreement_df['spacy_only_correct'].sum()\n",
    "bert_only_correct_count = agreement_df['bert_only_correct'].sum()\n",
    "\n",
    "print(f\"Total tokens analyzed: {total_tokens}\")\n",
    "print(f\"Systems agree: {systems_agree_count} ({systems_agree_count/total_tokens*100:.1f}%)\")\n",
    "print(f\"Both systems correct: {both_correct_count} ({both_correct_count/total_tokens*100:.1f}%)\")\n",
    "print(f\"Both systems wrong: {both_wrong_count} ({both_wrong_count/total_tokens*100:.1f}%)\")\n",
    "print(f\"Only spaCy correct: {spacy_only_correct_count} ({spacy_only_correct_count/total_tokens*100:.1f}%)\")\n",
    "print(f\"Only BERT correct: {bert_only_correct_count} ({bert_only_correct_count/total_tokens*100:.1f}%)\")\n",
    "\n",
    "# Show examples of disagreement\n",
    "print(\"\\nüîç Examples of System Disagreements:\")\n",
    "disagreements = agreement_df[~agreement_df['systems_agree']]\n",
    "if len(disagreements) > 0:\n",
    "    print(disagreements[['token', 'true_label', 'spacy_pred', 'bert_pred']].head(10))\n",
    "else:\n",
    "    print(\"No disagreements found in the test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d8148",
   "metadata": {},
   "source": [
    "## 9. Error Analysis and Examples\n",
    "\n",
    "### 9.1 Common Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "print(\"üîç ERROR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# spaCy Error Analysis\n",
    "spacy_errors = agreement_df[~agreement_df['spacy_correct']]\n",
    "print(f\"\\nüìä spaCy Errors ({len(spacy_errors)} total):\")\n",
    "if len(spacy_errors) > 0:\n",
    "    spacy_error_patterns = spacy_errors.groupby(['true_label', 'spacy_pred']).size().reset_index(name='count')\n",
    "    spacy_error_patterns = spacy_error_patterns.sort_values('count', ascending=False)\n",
    "    print(\"Most common error patterns:\")\n",
    "    print(spacy_error_patterns.head(10))\n",
    "    \n",
    "    print(\"\\nExample spaCy errors:\")\n",
    "    for _, row in spacy_errors.head(5).iterrows():\n",
    "        print(f\"  Token: '{row['token']}' | True: {row['true_label']} | Predicted: {row['spacy_pred']}\")\n",
    "\n",
    "# BERT Error Analysis\n",
    "bert_errors = agreement_df[~agreement_df['bert_correct']]\n",
    "print(f\"\\nüìä BERT Errors ({len(bert_errors)} total):\")\n",
    "if len(bert_errors) > 0:\n",
    "    bert_error_patterns = bert_errors.groupby(['true_label', 'bert_pred']).size().reset_index(name='count')\n",
    "    bert_error_patterns = bert_error_patterns.sort_values('count', ascending=False)\n",
    "    print(\"Most common error patterns:\")\n",
    "    print(bert_error_patterns.head(10))\n",
    "    \n",
    "    print(\"\\nExample BERT errors:\")\n",
    "    for _, row in bert_errors.head(5).iterrows():\n",
    "        print(f\"  Token: '{row['token']}' | True: {row['true_label']} | Predicted: {row['bert_pred']}\")\n",
    "\n",
    "# Entity type analysis\n",
    "print(\"\\nüìä Entity Type Performance:\")\n",
    "for entity_type in ['PER', 'ORG', 'LOC', 'MISC']:\n",
    "    b_entity = f'B-{entity_type}'\n",
    "    i_entity = f'I-{entity_type}'\n",
    "    \n",
    "    # Count entity occurrences\n",
    "    entity_tokens = agreement_df[agreement_df['true_label'].str.contains(entity_type, na=False)]\n",
    "    \n",
    "    if len(entity_tokens) > 0:\n",
    "        spacy_correct_entity = entity_tokens['spacy_correct'].sum()\n",
    "        bert_correct_entity = entity_tokens['bert_correct'].sum()\n",
    "        total_entity = len(entity_tokens)\n",
    "        \n",
    "        print(f\"  {entity_type}: spaCy {spacy_correct_entity}/{total_entity} ({spacy_correct_entity/total_entity*100:.1f}%), BERT {bert_correct_entity}/{total_entity} ({bert_correct_entity/total_entity*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example outputs for poster\n",
    "print(\"\\nüéØ EXAMPLE OUTPUTS FOR POSTER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, row in test_sentences_df.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    tokens = row['token']\n",
    "    true_labels = row['true_label']\n",
    "    \n",
    "    # Get predictions for this sentence\n",
    "    sentence_results = results_df[results_df['sentence_id'] == row['sentence_id']]\n",
    "    \n",
    "    print(f\"\\nüìù Example {i+1}: {sentence}\")\n",
    "    print(f\"{'Token':<15} {'True':<10} {'spaCy':<10} {'BERT':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for _, token_row in sentence_results.iterrows():\n",
    "        token = token_row['token']\n",
    "        true_label = token_row['true_label']\n",
    "        spacy_pred = token_row['spacy_pred']\n",
    "        bert_pred = token_row['bert_pred']\n",
    "        \n",
    "        # Add visual indicators for correctness\n",
    "        spacy_indicator = \"‚úÖ\" if spacy_pred == true_label else \"‚ùå\"\n",
    "        bert_indicator = \"‚úÖ\" if bert_pred == true_label else \"‚ùå\"\n",
    "        \n",
    "        print(f\"{token:<15} {true_label:<10} {spacy_pred:<10} {bert_pred:<10} {spacy_indicator} {bert_indicator}\")\n",
    "    \n",
    "    if i >= 2:  # Show only first 3 examples\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef01810",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Potential Improvements\n",
    "\n",
    "### 10.1 Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conclusions based on results\n",
    "print(\"üéØ KEY FINDINGS AND CONCLUSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Performance comparison\n",
    "better_system = \"spaCy\" if spacy_f1 > bert_f1 else \"BERT\"\n",
    "f1_difference = abs(spacy_f1 - bert_f1)\n",
    "\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(f\"‚Ä¢ Best performing system: {better_system}\")\n",
    "print(f\"‚Ä¢ F1-score difference: {f1_difference:.4f}\")\n",
    "print(f\"‚Ä¢ System agreement rate: {systems_agree_count/total_tokens*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüîç Training Data Insights:\")\n",
    "print(f\"‚Ä¢ CoNLL-2003: {len(conll_processed)} samples processed\")\n",
    "print(f\"‚Ä¢ WikiANN: {len(wikiann_processed)} samples processed\")\n",
    "print(f\"‚Ä¢ WNUT-17: {len(wnut_processed)} samples processed\")\n",
    "\n",
    "print(f\"\\nüí° Key Observations:\")\n",
    "if spacy_f1 > bert_f1:\n",
    "    print(f\"‚Ä¢ spaCy transformer model outperformed BERT by {f1_difference:.4f} F1-score\")\n",
    "    print(f\"‚Ä¢ spaCy showed better precision: {spacy_precision:.4f} vs {bert_precision:.4f}\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ BERT model outperformed spaCy by {f1_difference:.4f} F1-score\")\n",
    "    print(f\"‚Ä¢ BERT showed better precision: {bert_precision:.4f} vs {spacy_precision:.4f}\")\n",
    "\n",
    "if systems_agree_count/total_tokens > 0.8:\n",
    "    print(f\"‚Ä¢ High agreement between systems ({systems_agree_count/total_tokens*100:.1f}%) suggests consistent performance\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ Lower agreement between systems ({systems_agree_count/total_tokens*100:.1f}%) indicates different strengths\")\n",
    "\n",
    "print(f\"\\nüöÄ Potential Improvements:\")\n",
    "print(f\"‚Ä¢ Fine-tune models on domain-specific data\")\n",
    "print(f\"‚Ä¢ Ensemble methods combining both systems\")\n",
    "print(f\"‚Ä¢ Address common error patterns (e.g., {entity_type} entities)\")\n",
    "print(f\"‚Ä¢ Expand training data with more diverse examples\")\n",
    "print(f\"‚Ä¢ Post-processing rules for specific entity types\")\n",
    "\n",
    "print(f\"\\nüìà Recommendations for Future Work:\")\n",
    "print(f\"‚Ä¢ Evaluate on larger test sets\")\n",
    "print(f\"‚Ä¢ Test on different domains (medical, legal, social media)\")\n",
    "print(f\"‚Ä¢ Investigate cross-lingual performance\")\n",
    "print(f\"‚Ä¢ Analyze computational efficiency\")\n",
    "print(f\"‚Ä¢ Study the impact of text preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a01f6f",
   "metadata": {},
   "source": [
    "## üìã Summary for Academic Poster\n",
    "\n",
    "### Quick Reference Numbers:\n",
    "\n",
    "**Systems Compared:**\n",
    "- System 1: spaCy en_core_web_trf (Transformer-based)\n",
    "- System 2: BERT dslim/bert-base-NER\n",
    "\n",
    "**Training Datasets:**\n",
    "- CoNLL-2003: Standard NER benchmark\n",
    "- WikiANN: Multilingual Wikipedia-based dataset\n",
    "- WNUT-17: Emerging entities from social media\n",
    "\n",
    "**Performance Results:**\n",
    "- Best system accuracy and methodological approach comparison\n",
    "- Entity-level analysis (PERSON, ORGANIZATION, LOCATION, MISC)\n",
    "- Error pattern identification and system agreement analysis\n",
    "\n",
    "**Key Contributions:**\n",
    "- Comparative evaluation of two state-of-the-art NER systems\n",
    "- Analysis of system strengths and weaknesses\n",
    "- Identification of potential improvements for real-world deployment\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provides comprehensive analysis suitable for academic presentation and future NER system development.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
