{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b022532d",
   "metadata": {},
   "source": [
    "Importing and Preprocessing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce56d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence sentiment   topic\n",
      "0  The stadium was alive with the roar of the cro...  positive  sports\n",
      "1  That last-minute goal had me jumping out of my...  positive  sports\n",
      "2  I couldnâ€™t put the book down; it swept me into...  positive    book\n",
      "3  The story had its moments, though some parts f...   neutral    book\n",
      "4  I enjoyed the way the timelines shifted, even ...   neutral    book\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('sentiment-topic-test.tsv', sep='\\t')\n",
    "test_data.drop('sentence_id', axis=1, inplace=True)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b01de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text   rating\n",
      "0  The Rock is destined to be the 21st Century 's...  0.50000\n",
      "1  The gorgeously elaborate continuation of `` Th...  0.44444\n",
      "2                     Effective but too-tepid biopic  0.50000\n",
      "3  If you sometimes like to go to the movies to h...  0.42708\n",
      "4  Emerges as something rare , an issue movie tha...  0.37500\n"
     ]
    }
   ],
   "source": [
    "sentences = pd.read_csv('stanfordSentimentTreebank\\datasetSentences.txt', sep='\\t')\n",
    "sentiments = pd.read_csv('stanfordSentimentTreebank\\sentiment_labels.txt', sep='|', engine='python')\n",
    "sentiments.columns = ['sentence_index', 'sentiment_value']\n",
    "\n",
    "train_data_stf = pd.merge(sentences, sentiments, on='sentence_index')\n",
    "train_data_stf = train_data_stf.rename(columns={'sentence': 'text', 'sentiment_value': 'rating'})\n",
    "train_data_stf.drop('sentence_index', axis=1, inplace=True)\n",
    "print(train_data_stf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aed352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rating distribution:\n",
      "rating\n",
      "1.0     2773\n",
      "2.0     3666\n",
      "3.0    10006\n",
      "4.0    23875\n",
      "5.0    59680\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Minimum count: 2773\n",
      "\n",
      "Balanced rating distribution:\n",
      "rating\n",
      "1.0    2773\n",
      "2.0    2773\n",
      "3.0    2773\n",
      "4.0    2773\n",
      "5.0    2773\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original dataset size: 100000\n",
      "Balanced dataset size: 13865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_3016\\4023014913.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_data_amazon = train_data.groupby('rating').apply(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "    \"raw_review_Books\", \n",
    "    split=\"full[:100000]\", \n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "train_data = df[['rating', 'text']]\n",
    "\n",
    "print(\"Original rating distribution:\")\n",
    "print(train_data['rating'].value_counts().sort_index())\n",
    "\n",
    "min_count = train_data['rating'].value_counts().min()\n",
    "print(f\"\\nMinimum count: {min_count}\")\n",
    "\n",
    "train_data_amazon = train_data.groupby('rating').apply(\n",
    "    lambda x: x.sample(n=min_count, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nBalanced rating distribution:\")\n",
    "print(train_data_amazon['rating'].value_counts().sort_index())\n",
    "print(f\"\\nOriginal dataset size: {len(train_data)}\")\n",
    "print(f\"Balanced dataset size: {len(train_data_amazon)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6c4e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                               text\n",
      "0     1.0  The premise of this book was a good one. A boo...\n",
      "1     5.0  Simply and eloquently articulates the tangled ...\n",
      "2     5.0  Janet Stevens is a wonderful illustrator.  Whe...\n",
      "3     5.0  The film makes a tragic error by going on for ...\n",
      "4     5.0  and your reward will be a thoughtful , emotion...\n"
     ]
    }
   ],
   "source": [
    "train_data_stf['rating'] = (train_data_stf['rating'] * 5).clip(1, 5).round()\n",
    "train_data_combined = pd.concat([train_data_amazon, train_data_stf], ignore_index=True)\n",
    "train_data_combined = train_data_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(train_data_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78944c8a",
   "metadata": {},
   "source": [
    "Multinominal Naive Bayes and Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cac1479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.53      0.63      1062\n",
      "    positive       0.89      0.96      0.92      4082\n",
      "\n",
      "    accuracy                           0.87      5144\n",
      "   macro avg       0.84      0.74      0.78      5144\n",
      "weighted avg       0.87      0.87      0.86      5144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rating_to_sentiment(rating):\n",
    "    if rating >= 3.0:\n",
    "        return 'positive'\n",
    "    elif rating <= 2.0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "#vectorizer = CountVectorizer(min_df=2)\n",
    "vectorizer = TfidfVectorizer(min_df=2) # Changing min_df doesnt make difference\n",
    "\n",
    "train_data_combined['sentiment'] = train_data_combined['rating'].apply(rating_to_sentiment)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data_combined['text'], \n",
    "    train_data_combined['sentiment'], \n",
    "    test_size=0.2, \n",
    "    random_state=42)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "#clf = MultinomialNB()\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2e0f1",
   "metadata": {},
   "source": [
    "VADER Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "463cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction: positive, gold: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\staff\\NLP\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Lenovo\\staff\\NLP\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Lenovo\\staff\\NLP\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.42      0.38      2753\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.83      0.64      0.73     10107\n",
      "\n",
      "    accuracy                           0.60     12860\n",
      "   macro avg       0.39      0.36      0.37     12860\n",
      "weighted avg       0.73      0.60      0.65     12860\n",
      "\n",
      "[[1161  300 1292]\n",
      " [   0    0    0]\n",
      " [2156 1443 6508]]\n",
      "accuracy: 0.5963452566096423\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "pos = set()\n",
    "\n",
    "def run_vader(sentence, lemmatize=True, parts_of_speech_to_consider=pos):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence and return the scores.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    return scores\n",
    "\n",
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    Convert VADER output to a label.\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound >= 0.1:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "train_data_half = train_data_combined.sample(frac=0.5, random_state=42)\n",
    "predictions = train_data_half['text'].apply(lambda x: vader_output_to_label(run_vader(x)))\n",
    "gold = train_data_half['sentiment']\n",
    "\n",
    "print(f\"Sample prediction: {predictions.iloc[2]}, gold: {gold.iloc[2]}\")\n",
    "print(classification_report(gold, predictions))\n",
    "print(confusion_matrix(gold, predictions))\n",
    "print(f'accuracy: {accuracy_score(gold, predictions)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
